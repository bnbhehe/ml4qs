{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cts/anaconda2/lib/python2.7/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "%pylab inline\n",
    "import pandas as pd\n",
    "import json\n",
    "import seaborn\n",
    "import copy\n",
    "import os\n",
    "from Chapter2.CreateDataset import CreateDataset\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from Chapter3.OutlierDetection import DistributionBasedOutlierDetection\n",
    "from Chapter3.OutlierDetection import DistanceBasedOutlierDetection\n",
    "from util.VisualizeDataset import VisualizeDataset\n",
    "from util import util\n",
    "from util.parser import get_labels,generate_csv,generate_labels,get_sensor_values\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below cell has global variables (path-names mostly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['log_assignment2', 'csv', 'log_events_assignment2']\n"
     ]
    }
   ],
   "source": [
    "dataset_folder = './datasets'\n",
    "files = os.listdir(dataset_folder)\n",
    "print(files)\n",
    "dataset_fname  = 'log_assignment2'\n",
    "event_fname = 'log_events_assignment2'\n",
    "result_dataset_path = './intermediate_datafiles/'\n",
    "csv_dataset_path = os.path.join(dataset_folder,'csv/')\n",
    "dataset_path = os.path.join(dataset_folder,dataset_fname)\n",
    "event_path = os.path.join(dataset_folder,event_fname)\n",
    "output_fname = 'dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(result_dataset_path):\n",
    "    print('Creating result directory: ' + result_dataset_path)\n",
    "    os.makedirs(result_dataset_path)\n",
    "if not os.path.exists(csv_dataset_path):\n",
    "    print('Creating result directory: ' + csv_dataset_path)\n",
    "    os.makedirs(csv_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOG PARSING FUNCTION CALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mappings:\n",
      "\n",
      "{\n",
      "  \"standing\": [\n",
      "    6, \n",
      "    11\n",
      "  ], \n",
      "  \"walking\": [\n",
      "    9, \n",
      "    12\n",
      "  ], \n",
      "  \"sitting\": [\n",
      "    10, \n",
      "    13\n",
      "  ], \n",
      "  \"stairsdown\": [\n",
      "    4, \n",
      "    7\n",
      "  ], \n",
      "  \"stairsup\": [\n",
      "    5, \n",
      "    8\n",
      "  ], \n",
      "  \"ondesk\": [\n",
      "    14\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "labels = get_labels(dataset_path)\n",
    "print('Label Mappings:\\n')\n",
    "print(json.dumps(labels,indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensors in log file\n",
      "[\n",
      "  \"bmi160 accelerometer\", \n",
      "  \"bmi160 gyroscope\", \n",
      "  \"rotation vector\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "sensors_dict = get_sensor_values(dataset_path)\n",
    "print('Sensors in log file')\n",
    "sensors = sensors_dict.keys()\n",
    "print(json.dumps(sensors,indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV generation cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sensor in sensors:\n",
    "    if sensor == 'rotation vector':\n",
    "        generate_csv(sensors_dict[sensor],\n",
    "                     header=['x','y','z','theta','phi','timestamp'],\n",
    "                     fname = os.path.join(csv_dataset_path,sensor+'.csv'))\n",
    "    else:\n",
    "        generate_csv(sensors_dict[sensor],fname = os.path.join(csv_dataset_path,sensor+'.csv'))\n",
    "generate_labels(fname=event_path,log_fname=dataset_path,fout=csv_dataset_path+'labels.csv');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "granularity is 250 ms\n",
      "Added all sensors and labels\n",
      "Dataset created,\tDuration:18.89590 seconds\n",
      "====================================\n",
      "granularity is 1000 ms\n",
      "Added all sensors and labels\n",
      "Dataset created,\tDuration:5.60809 seconds\n",
      "====================================\n",
      "granularity is 10000 ms\n",
      "Added all sensors and labels\n",
      "Dataset created,\tDuration:1.76530 seconds\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "granularities = [250,1000,10000]\n",
    "datasets = []\n",
    "\n",
    "\n",
    "for milliseconds_per_instance in granularities:\n",
    "    initial = time.time()\n",
    "    # Create an initial dataset object with the base directory for our data and a granularity\n",
    "    DataSet = CreateDataset(csv_dataset_path, milliseconds_per_instance)\n",
    "    print('granularity is %d ms'%milliseconds_per_instance)\n",
    "    \n",
    "    DataSet.add_numerical_dataset('bmi160 accelerometer.csv','timestamp',['x','y','z'],'avg','acc_phone_')\n",
    "    DataSet.add_numerical_dataset('bmi160 gyroscope.csv','timestamp',['x','y','z'],'avg','gyr_phone_')\n",
    "    DataSet.add_numerical_dataset('rotation vector.csv','timestamp',['x','y','z','theta','phi'],'avg','rotation_phone_')\n",
    "    DataSet.add_event_dataset('labels.csv','label_start','label_end','label','binary')\n",
    "    print('Added all sensors and labels')\n",
    "    dataset = DataSet.data_table\n",
    "#     util.print_statistics(dataset)\n",
    "    datasets.append(copy.deepcopy(dataset))\n",
    "    dataset.to_csv(result_dataset_path + '/'+str(milliseconds_per_instance)+ '_dataset.csv')\n",
    "    print('Dataset created,\\tDuration:%.5f seconds'%(time.time()-initial))\n",
    "    print('====================================')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
